import os
os.getcwd()














import yfinance as yf
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats


tickers = [
    "USDTRY=X",
    "XU100.IS",
    "^VIX",
    "USDZAR=X",
    "USDMXN=X",
    "USDBRL=X"
]

data = yf.download(
    tickers,
    start="2018-01-01",
    end="2025-01-01",
    interval="1d",
    auto_adjust=True
)



clean_data = data["Close"].copy().dropna(how="all")


clean_data.head()


# Creating a separate table for returns

returns = clean_data.pct_change(fill_method=None)


# We drop missing values in our returns table and just take a look

returns = returns.dropna()
returns.head()


# Let's compute the correlation matrix of returns
plt.figure(figsize=(8,6))
sns.heatmap(returns.corr(), annot=True, fmt=".2f", cmap="coolwarm", center=0)
plt.title("Correlation Matrix of Daily Returns")
plt.tight_layout()
plt.show()





# Let's now detect days when the Turkish Lira experienced a large shock.

# A daily FX move of 3% is economically large for a major currency, and itâ€™s commonly used in crisis/event studies as a shock threshold

crisis_days = returns[returns["USDTRY=X"] <= -0.03]
crisis_days.head()


# Now let's extract windows around each TRY crisis day, 5 days before (for the crisis buildup) and 5 days after (for the spillover effects)
event_windows = []

for date in crisis_days.index:
    window = returns.loc[date - pd.Timedelta(days=5): date + pd.Timedelta(days=5)]
    event_windows.append(window)

# here We deduplicated the event windows to remove overlapping days shared between nearby crisis events,
# ensuring each date is counted only once in the crisis correlation matrix.

event_windows = pd.concat(event_windows)
event_windows = event_windows[~event_windows.index.duplicated(keep='first')]

event_windows.head()


# Now comparing correlations inside these windows vs normal periods

crisis_corr = event_windows.corr()
crisis_corr


# to visualize it

plt.figure(figsize=(8,6))
sns.heatmap(crisis_corr, annot=True, fmt=".2f", cmap="coolwarm", center=0)
plt.title("Correlation Matrix During TRY Crisis Windows")
plt.tight_layout()
plt.show()








# The first 29 rows of the table will be all NaNs. That's because this table is for rolling correlations of a month (30 days)

rolling_corr = {}

pairs = ["USDBRL=X", "USDMXN=X", "USDZAR=X", "XU100.IS"]

for pair in pairs:
    rolling_corr[pair] = returns["USDTRY=X"].rolling(window=30).corr(returns[pair])

rolling_corr_df = pd.DataFrame(rolling_corr)
rolling_corr_df.head()


fig, axes = plt.subplots(4, 1, figsize=(12, 12), sharex=True)

for ax, pair in zip(axes, pairs):
    ax.plot(rolling_corr_df.index, rolling_corr_df[pair], label=pair)
    for date in crisis_days.index:
        ax.axvline(x=date, color='red', alpha=0.2, linewidth=0.8)
    ax.axhline(y=0, color='black', linewidth=0.5, linestyle='--')
    ax.set_title(f"TRY vs {pair}")
    ax.legend()

plt.suptitle("30-Day Rolling Correlations with USDTRY", fontsize=14)
plt.tight_layout()
plt.show()








# Comparing rolling correlation values during crisis windows vs. normal periods 
# This is to check if the difference is statistically significant (p<0.05 = significant contagion).

normal_returns = returns[~returns.index.isin(event_windows_clean.index)]

for pair in pairs:
    crisis_vals = rolling_corr_df.loc[event_windows_clean.index, pair].dropna()
    normal_vals = rolling_corr_df.loc[normal_returns.index, pair].dropna()
    t_stat, p_val = stats.ttest_ind(crisis_vals, normal_vals)
    print(f"TRY vs {pair}: t={t_stat:.3f}, p={p_val:.3f}")


fig, axes = plt.subplots(2, 2, figsize=(12, 8))

for ax, pair in zip(axes.flatten(), pairs):
    crisis_vals = rolling_corr_df.loc[event_windows_clean.index, pair].dropna()
    normal_vals = rolling_corr_df.loc[normal_returns.index, pair].dropna()
    ax.boxplot([normal_vals, crisis_vals], tick_labels=["Normal", "Crisis"])
    ax.set_title(f"TRY vs {pair}")
    ax.axhline(y=0, color='red', linewidth=0.5, linestyle='--')

plt.suptitle("Rolling Correlation Distributions: Crisis vs Normal Periods", fontsize=13)
plt.tight_layout()
plt.show()








# Defining an  Estimation Window 
# We select a period of 'normal' market behavior BEFORE the crises begin, so that we dont' include crisis days

estimation_start = '2017-01-01'
estimation_end   = '2017-12-31'
estimation_window = returns.loc[estimation_start:estimation_end]

# Calculating average normal return from the CLEAN baseline only
normal_mean = estimation_window[pairs].mean()

# Calculating abnormal returns for each crisis window day. This compares crisis days to the 'calm' baseline
abnormal_returns = event_windows_clean[pairs] - normal_mean

# Calculating CAR by averaging across all crisis events
CAR = abnormal_returns.groupby(level=0).sum()
avg_CAR = CAR.mean()

print(f"Average Cumulative Abnormal Returns (Baseline: {estimation_start} to {estimation_end}):")
print(avg_CAR)








avg_CARs = []
errors = []

for pair in pairs:
    event_CARs = []
    for date in crisis_days.index:
        window = returns.loc[date - pd.Timedelta(days=5): date + pd.Timedelta(days=5)]
        car = (window[pair] - normal_mean[pair]).sum()
        event_CARs.append(car)
    avg_CARs.append(sum(event_CARs)/len(event_CARs)*100)
    errors.append(pd.Series(event_CARs).std() / len(event_CARs)**0.5 * 100)

plt.figure(figsize=(8, 5))
plt.bar(pairs, avg_CARs, yerr=errors, capsize=5, color=["red" if x < 0 else "green" for x in avg_CARs])
plt.axhline(y=0, color='black', linewidth=0.8, linestyle='--')
plt.title("Average Cumulative Abnormal Returns During TRY Crisis Windows")
plt.ylabel("CAR (%)")
plt.tight_layout()
plt.show()



























